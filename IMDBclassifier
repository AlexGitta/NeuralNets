# edited to avoid overfitting

import os
import numpy as np
import tensorflow as tf

import matplotlib.pyplot as plt
plt.rcParams['lines.markersize'] = 2  # nice scatter point size

# matplotlib shenanigans
import matplotlib
plt_fontsize = matplotlib.rcParams["font.size"]

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # mute some annoying warnings

# download data                            # top 10000 most frequent words only, discard rare words 
((train_data, train_labels), (test_data, test_labels)) = tf.keras.datasets.imdb.load_data(num_words=10000)
print(train_data.shape, test_data.shape)

# get words → index
word_index = tf.keras.datasets.imdb.get_word_index()
# reverse: index → word (Python dict comprehension)
reverse_word_index = {value:key for key,value in word_index.items()}

# turn our texts into multi-hot encodings
def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))     # a matrix of shape len(seq) x vocab, full of zeros
    for i, sequence in enumerate(sequences):            # for each sequence:
        results[i, sequence] = 1.                       # fill the appropriate indices with 1
    return results                                      # note the NumPy magic! `sequence` is an array!
                                                        # acting as the list of all indices where we want 1s

# one hot encoding

x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

y_train = np.asarray(train_labels).astype('float32') # convert to float32
y_test = np.asarray(test_labels).astype('float32')

# setup net

model = tf.keras.models.Sequential()
model.add(tf.keras.Input((10000,)))
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dense(16 ,activation='relu'))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

# compile model

model.compile(
    optimizer='rmsprop',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# train model
 
model.fit(
    x_train,                     # full training set
    y_train,
    epochs=early_stopping_epoch, # the epoch where overfitting starts
    batch_size=512
)

# test

results = model.evaluate(x_test, y_test, verbose=0)

print(f"loss: {results[0]}, accuracy: {results[1]}") # accuracy better than before!

# now we can actually use the net to predict how positive/negative reviews are

model.predict(x_test[:10], verbose=0) # this yields probabilities: negative (0) or positive (1) ?

def print_predictions(preds, init=0):
    labels = ["negative", "positive"]
    for i, pred in enumerate(preds.ravel()): # ravel() makes the array flat (1D)
        print(f"Review n° {init+i} is {labels[int(pred > .5)]} | score: {pred:.5f}")

# print out all reviews and scores

print_predictions(model.predict(x_test[:10], verbose=0))

# function to find reviews

def review_finder(prediction, label):
    for i in range(len(x_test)):                                       # looping through our test set
        y_pred = round(model.predict(x_test[i:i+1], verbose=0)[0, 0])  # get prediction
        if y_pred == prediction and test_labels[i] == label:           # if the prediction and the label
            yield i                                                    # are what we are after, yield the index

def print_review(n):
    word_index = tf.keras.datasets.imdb.get_word_index()                                                  # decoding work
    reverse_word_index = {value:key for key,value in word_index.items()}
    decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in test_data[n]])

    y_pred = model.predict(x_test[n:n+1], verbose=0)[0,0]                                                 # predict
    print(f"review: {n} | predicted: {y_pred:.6f} | rounded: {round(y_pred)} | label: {test_labels[n]}")  # print
    print()
    print(decoded_review, '\n')

# find correctly classified reviews

neg_neg = review_finder(prediction=0, label=0) # create a generator
pos_pos = review_finder(prediction=1, label=1) # another generator#

print_review(next(neg_neg)) # run again for the next one
print_review(next(pos_pos))

# find the reviews that are wrongly classified

neg_pos = review_finder(prediction=0, label=1)
pos_neg = review_finder(prediction=1, label=0)

print_review(next(neg_pos))
print_review(next(pos_neg))

